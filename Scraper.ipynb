{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from Queue import Queue\n",
    "\n",
    "try:\n",
    "    import cPickle as pkl\n",
    "except ImportError:\n",
    "    import pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_links(category, pre_existing_queue=Queue(), url=None):\n",
    "    \"\"\"\n",
    "    Generates a list of articles and sub-categories in a category on Wikipedia\n",
    "\n",
    "    category: category name as a string\n",
    "    pre_existing_queue: (Optional) queue of subcategories, if provided this code will append to it\n",
    "    url: url to go to\n",
    "\n",
    "    return: tuple of (list of links, queue of sub-categories)\n",
    "    \"\"\"\n",
    "\n",
    "    def get_category_url(category_name):\n",
    "        category_prefix = \"https://en.wikipedia.org/wiki/Category:\"\n",
    "        return category_prefix + category_name\n",
    "\n",
    "    def get_absolute_url(wikipedia_relative_url):\n",
    "        return \"https://en.wikipedia.org\" + wikipedia_relative_url\n",
    "\n",
    "    if url is None:\n",
    "        url = get_category_url(category)\n",
    "\n",
    "    html_category_page = requests.get(url).text\n",
    "    parsed_category_page = BeautifulSoup(html_category_page, \"html.parser\")\n",
    "    links = parsed_category_page.findAll(\"div\", {\"class\":\"mw-category-group\"})\n",
    "\n",
    "    # Initialize the pages list and subcategory queue\n",
    "    pages = []\n",
    "    subcategory_queue = pre_existing_queue\n",
    "\n",
    "    # Add all \"links to pages\" to the pages list, and all \"links to subcategories\" to the subcategory queue\n",
    "    for item in links:\n",
    "        for link_tag in item.select(\"a\"):\n",
    "            link_url = link_tag[\"href\"]\n",
    "            if \"Category:\" in link_url:\n",
    "                subcategory_queue.put(get_absolute_url(link_url))\n",
    "            else:\n",
    "                pages.append(get_absolute_url(link_url))\n",
    "\n",
    "    # If this category listing has more pages, then proceed to them\n",
    "    last_text_on_page = parsed_category_page.findAll(\"div\", {\"class\":\"mw-content-ltr\"})[0].select(\"a\")[-1].text\n",
    "    if last_text_on_page == \"next page\":\n",
    "        next_page_url = get_absolute_url(parsed_category_page.findAll(\"div\", {\"class\":\"mw-content-ltr\"})[0].select(\"a\")[-1][\"href\"])\n",
    "        get_links(category, subcategory_queue, next_page_url)\n",
    "\n",
    "    return pages, subcategory_queue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_article_text_and_metadata(url):\n",
    "    \"\"\"\n",
    "    Fetches and cleans a Wikipedia article\n",
    "    url: URL of the Wikipedia article\n",
    "    returns tuple of (plaintext article content, number_of_images, number_of_internal_links, number_of_citations)\n",
    "    \"\"\"\n",
    "    article_content = \"\"\n",
    "\n",
    "    # Fetch the webpage and parse it\n",
    "    raw_html = requests.get(url).text\n",
    "    parsed_html = BeautifulSoup(raw_html, \"html.parser\")\n",
    "    number_of_citations = 0\n",
    "\n",
    "    for html_paragraph in parsed_html.select(\"div p\"):\n",
    "\n",
    "        # Strip citations from the text and count them\n",
    "        for citation in html_paragraph.find_all(\"sup\"):\n",
    "            citation.replaceWith(\" \")\n",
    "            number_of_citations += 1\n",
    "\n",
    "        # Convert the HTML to text and strip out punctuation\n",
    "        text_paragraph = html_paragraph.getText().lower()\n",
    "        cleaned_paragraph = filter(lambda char: char.isalnum() or char == \" \", text_paragraph)\n",
    "        article_content += \" \" + cleaned_paragraph\n",
    "    \n",
    "    images = parsed_html.select(\"img\")\n",
    "    number_of_images = len(images)\n",
    "    number_of_tex = 0\n",
    "    for item in images:\n",
    "        try:\n",
    "            tmp = item['class']\n",
    "        except KeyError:\n",
    "            tmp = []\n",
    "        if \"mwe-math-fallback-image-inline\" in tmp:\n",
    "            number_of_tex += 1\n",
    "            \n",
    "    number_of_nonTexImages = number_of_images - number_of_tex\n",
    "    number_of_internal_links = len(parsed_html.find(\"div\", {\"id\":\"bodyContent\"}).select(\"a\")) - number_of_citations\n",
    "    \n",
    "    return article_content, number_of_nonTexImages, number_of_tex, number_of_internal_links, number_of_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting links for Rare_diseases\n",
      "Rare_diseases 7558\n",
      "Collecting links for Infectious_diseases\n",
      "Infectious_diseases 9852\n",
      "Rare_diseases 7558\n",
      "Collecting links for Cancer\n",
      "Infectious_diseases 9852\n",
      "Rare_diseases 7558\n",
      "Cancer 2329\n",
      "Collecting links for Congenital_disorders\n",
      "Infectious_diseases 9852\n",
      "Congenital_disorders 1200\n",
      "Rare_diseases 7558\n",
      "Cancer 2329\n",
      "Collecting links for Organs_(anatomy)\n",
      "Infectious_diseases 9852\n",
      "Congenital_disorders 1200\n",
      "Rare_diseases 7558\n",
      "Cancer 2329\n",
      "Organs_(anatomy) 100078\n",
      "Collecting links for Machine_learning_algorithms\n",
      "Congenital_disorders 1200\n",
      "Rare_diseases 7558\n",
      "Cancer 2329\n",
      "Infectious_diseases 9852\n",
      "Machine_learning_algorithms 100053\n",
      "Organs_(anatomy) 100078\n",
      "Collecting links for Medical_devices\n",
      "Congenital_disorders 1200\n",
      "Rare_diseases 7558\n",
      "Cancer 2329\n",
      "Infectious_diseases 9852\n",
      "Medical_devices 100119\n",
      "Machine_learning_algorithms 100053\n",
      "Organs_(anatomy) 100078\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nwith open(\"all_pages.pkl\", \"r\") as f:\\n    all_pages = pkl.load(f)\\n\\nfor category_name in all_pages:\\n    print \"Downloading\", len(all_pages[category_name]), \"pages about\", category_name\\n    count = 0\\n    for url in all_pages[category_name]:\\n        count += 1\\n        if count%1000 == 0:\\n            print count, category_name\\n        articles.append(list(get_article_text_and_metadata(url)) + [category_name])\\n\\n    filename = category_name + \"_data.pkl\"\\n    pkl.dump(articles, open(filename, \\'wb\\'))\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "main_categories = [\"Rare_diseases\",\n",
    "                   \"Infectious_diseases\",\n",
    "                   \"Cancer\", \n",
    "                   \"Congenital_disorders\",\n",
    "                   \"Organs_(anatomy)\",\n",
    "                   \"Machine_learning_algorithms\",\n",
    "                   \"Medical_devices\"]\n",
    "\n",
    " \n",
    "all_pages = {}\n",
    "max_pages = 100000\n",
    "\n",
    "for category_name in main_categories:\n",
    "    print \"Collecting links for\", category_name\n",
    "    pages, subcategories = get_links(category_name)\n",
    "    all_pages[category_name] = pages\n",
    "    while len(all_pages[category_name]) < max_pages and subcategories.qsize() > 0:\n",
    "        subcategory_url = subcategories.get()\n",
    "        pages, subcategories = get_links(category_name, subcategories, subcategory_url)\n",
    "        all_pages[category_name] += pages\n",
    "        \n",
    "    for key in all_pages:\n",
    "        print key, len(all_pages[key])\n",
    "'''\n",
    "with open(\"all_pages.pkl\", \"r\") as f:\n",
    "    all_pages = pkl.load(f)\n",
    "\n",
    "for category_name in all_pages:\n",
    "    print \"Downloading\", len(all_pages[category_name]), \"pages about\", category_name\n",
    "    count = 0\n",
    "    for url in all_pages[category_name]:\n",
    "        count += 1\n",
    "        if count%1000 == 0:\n",
    "            print count, category_name\n",
    "        articles.append(list(get_article_text_and_metadata(url)) + [category_name])\n",
    "\n",
    "    filename = category_name + \"_data.pkl\"\n",
    "    pkl.dump(articles, open(filename, 'wb'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
